# Amphi 6
## Codes, codages, entropie, compression

## Codage:
C'est passer d'une représentation à une autre, ex: de l'alphabet à un code binaire.  
Soit $m$ un motif (un mot) tel que $m = (m_0m_1 \ldots m_{l-1})$.  
Soit $\tau$: $A$ $\rightarrow$ $B^{k}$ un codage de l'ensemble des lettres $A$, à une représentation binaire $B$ de longueur $k$.  
- C'est un **morphisme**: il respecte la concaténation:

$$ \tau(m)=\tau(m_0)\tau(m_1)...\tau(m_{l-1}) $$

- Le codage est **injectif**: chaque code admet au plus un antécédant (une lettre par exemple).

**Exemple de code injectif**:

$$
\begin{align*}
\tau(a) = 00 \\
\tau(b) = 01 \\
\tau(c) = 10 \\
\tau(d) = 11
\end{align*}
$$

**Exemple de code non injectif**:

$$
\begin{align*}
\tau(a) = 0 \\
\tau(b) = 01 \\
\tau(c) = 10 \\
\end{align*}
$$

## Compression de données

On distingue deux types de codes:
___

### Codes de longueur fixe
Chaque lettre est codée sur des éléments de même longueur.    
Soit $\tau$: $A$ $\rightarrow$ $B^{k}$
un code de longueur $k$ fixe, alors pour tout mot $m \in A$, on a:

$$ 
\begin{align*}
\exists k, \tau(A) \subseteq B^k 
\end{align*}
$$

Pour pouvoir coder A dans B, il faut que 

$$k \geq log_{|B|}(|A|)$$

(|A| = Nombre de symboles dans A, idem pour B).

**Exemple**:  
Le **code ASCII** est un code de longueur fixe.

**Exemple**:

$$
\begin{align*}
\tau(a) = 00 \\
\tau(b) = 01 \\
\tau(c) = 10 \\
\tau(d) = 11
\end{align*}
$$  

Ici :  
$B = ${ $0,1$ }, $A =$ { $a,b,c,d$ }  
$log_{|B|}(|A|) = log_2(4) = 2$.  
On code sur 2 bits: $k = 2$
___

### Codes de longueur variable
Chaque lettre est codée sur des éléments de longueur variable.

- Ces codes sont utiles lorsque certaines lettres sont plus fréquentes que d'autres.
- Ils sont en général plus difficiles à construire et/ou décoder.

Les **codes préfixes** sont les plus faciles à construire: Ce sont des codes de longueur variable où **aucun mot codé n'est préfixe d'un autre mot codé**.
(C'est "le peigne" dans l'arbre de décodage)  

**Exemple**: 
Le **code morse** est un code de longueur variable.

**Exemple**:

$$
\begin{align*}
\tau(a) = 0 \\
\tau(b) = 10 \\
\tau(c) = 110 \\
\tau(d) = 111
\end{align*}
$$

Exemple de code non préfixe (mais de code suffixe !):

$$
\begin{align*}
\tau(a) = 0 \\
\tau(b) = 01 \\
\tau(c) = 011 \\
\end{align*}
$$


<div style="margin-top: 100px">

## Code compresseur
C'est un code qui permet d'obtenir une représentation plus compacte, pour:
- Transmettre plus rapidement.
- Stocker en utilisant moins d'espace.

Deux types de compression:
- **Avec perte**: on perd de l'information, ex: JPEG, MP3
- **Sans perte**: on ne perd pas d'information, ex: ZIP, PNG, FLAC

**Exemple** avec {a, b, c} en binaire:
- $a$ très fréquent, $b$ fréquent, $c$ peu fréquent.
- Code de longueur variable:

$$
\begin{align*}
\tau(a) = 0 \\
\tau(b) = 10 \\
\tau(c) = 11 \\
\end{align*}
$$

Ainsi, le mot "abacaba" est codé par "0100110010" (10 bits).  
Un code de longueur fixe aurait codé chaque lettre sur 2 bits, soit "00 01 00 11 00 01 00" (14 bits).

<div style="margin-top: 100px">


## Mesure de performance

**Quotient de compression**:

$$\text{q} = \frac{\text{Taille du message original}}{\text{Taille du message compressé}}$$

**Taux de compression**:

$$\tau = 1 - \frac{\text{Taille du message compressé}}{\text{Taille du message original}} = 1 - \frac{1}{\text{q}}$$


<div style="margin-top: 100px">

## Codage de Huffman
Algorithme pour construire un code préfixe optimal (minimisant la taille du message codé) en fonction des fréquences des symboles.

On construit l'arbre de Huffman, selon l'algorithme:
- On part de la **forêt** des lettres avec leur fréquence.
- On **regroupe les deux arbres de plus faible fréquence** tant qu'il reste au moins deux arbres dans la forêt.
- Le parent de ces deux arbres a pour fréquence la somme des fréquences de ses deux fils.

$\rightarrow$ Les lettres les plus fréquentes ont les codes les plus courts.

<div style="margin-top: 50px">

**Exemple** avec le mot "abracadabra":
Fréquence des lettres:  
$a$: $5/11$,  
$b$: $2/11$,  
$r$: $2/11$,  
$c$: $1/11$,  
$d$: $1/11$,  

Arbre de Huffman (0 à gauche, 1 à droite):
```
            (11)
           /    \
          /      \
        a(5)     (6)
                /   \
               /     \
             (2)     (4)
            /  \    /  \
           c    d  b    r
          (1)  (1)(2)  (2)
```

Codes obtenus:

$$
\begin{align*}
\tau(a) = 0 \\
\tau(c) = 100 \\
\tau(d) = 101 \\
\tau(b) = 110 \\
\tau(r) = 111 \\
\end{align*}
$$

<div style="margin-top: 50px">


# Entropie de Shannon
L'entropie de Shannon mesure la quantité moyenne d'information par symbole dans un message.

$$H = - \sum_{i=1}^{n} p_i \log_2(p_i) $$

où $p_i$ est la probabilité d'apparition du symbole $i$.

**Exemple** avec la météo:
- Soleil (S): 50% (0.5)
- Nuage (N): 30% (0.3)
- Pluie (P): 20% (0.2)
- $n = 3$

$$H = - (0.5 \log_2(0.5) + 0.3 \log_2(0.3) + 0.2 \log_2(0.2)) \approx 1.485$$

Donc en moyenne, chaque symbole de la météo apporte environ **1.485 bits d'information**.
